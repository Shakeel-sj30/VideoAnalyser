App  link:https://shakeel-sj30-videoanalyser-streamlit-app-gr5xst.streamlit.app/
# GenTA Mini GACS Prototype: Affective Computing for Art & Marketing Visuals

A verification-first AI R&D pipeline for understanding the "vibe" (mood/style) of contemporary art and marketing videos through multimodal embeddings and similarity analysis.

**Status:** âœ“ Production-ready | Verified | Tested

---

## ğŸ“‹ Overview

This repository contains a complete, reproducible pipeline for:

1. **Frame Extraction** â†’ Extract representative frames from video files with metadata
2. **Embedding Generation** â†’ Compute CLIP multimodal embeddings for mood/style representation
3. **Similarity Analysis** â†’ Identify frames with similar aesthetic/emotional properties
4. **Visualization & Reporting** â†’ Generate heatmaps, charts, and similarity reports
5. **Verification Testing** â†’ Comprehensive assertions ensuring data integrity

**GenTA Context:** This prototype demonstrates core capabilities for a GACS-like affective computing engine that could eventually integrate performance feedback (CTR/ROAS) for creative optimization.

---

## ğŸ—ï¸ Project Structure

```
vibe_project/
â”‚
â”œâ”€â”€ videos/                          # Input: AI marketing or art videos (add your .mp4 files here)
â”œâ”€â”€ frames/                          # Output: Extracted frames + metadata.json
â”œâ”€â”€ embeddings/                      # Output: .npy embeddings + index mappings
â”œâ”€â”€ outputs/                         # Output: Visualizations & reports
â”‚
â”œâ”€â”€ extract_frames.py               # Stage 1: Frame extraction from videos
â”œâ”€â”€ embed_frames.py                 # Stage 2: CLIP embeddings generation
â”œâ”€â”€ similarity_heatmap.py           # Stage 3: Similarity computation & analysis
â”œâ”€â”€ verification_tests.py           # Testing & validation suite
â”œâ”€â”€ utils.py                        # Common utilities (logging, config, metrics)
â”‚
â”œâ”€â”€ GenTA_Affective_Computing_Pipeline.ipynb   # Full interactive walkthrough
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ .gitignore                      # Git ignore rules
```

---

## âš™ï¸ Setup & Installation

### Prerequisites
- **Python 3.8+**
- **CUDA 11.0+** (optional, for GPU acceleration)
- **4-8GB RAM** recommended

### Step 1: Clone & Environment Setup

```bash
# Navigate to project directory
cd vibe_project

# Create virtual environment
python -m venv venv
source venv/bin/activate          # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Verify Installation

```bash
python utils.py    # Check dependencies and system info
```

Expected output:
```
âœ“ torch: available
âœ“ transformers: available
âœ“ opencv-python: available
...
Device: cuda (if GPU available)
```

---

## ğŸ¬ Quick Start: Running the Pipeline

### Option A: Sequential Scripts (Recommended for Production)

```bash
# Step 1: Extract frames from all videos in ./videos/
python extract_frames.py

# Step 2: Generate embeddings for all frames
python embed_frames.py

# Step 3: Compute similarity and generate visualizations
python similarity_heatmap.py

# Step 4: Run verification tests
python verification_tests.py
```

**Output Locations:**
- Frames: `./frames/` (+ `metadata.json`)
- Embeddings: `./embeddings/` (+ `index_mapping.json`)
- Reports: `./outputs/` (heatmaps, charts)
- Logs: Console + optional `logs/` directory

---

### Option B: Interactive Jupyter Notebook

For exploration and visualization:

```bash
jupyter notebook GenTA_Affective_Computing_Pipeline.ipynb
```

This notebook includes:
- Step-by-step pipeline execution
- Inline visualizations
- Detailed explanations of results
- Verification test results

---

## ğŸŒ Share with Anyone (Public URL)

### Recommended: Streamlit Community Cloud (Free)

This gives you a permanent public URL like:
`https://your-app-name.streamlit.app`

1. Push this project to a GitHub repository.
2. Go to [https://share.streamlit.io](https://share.streamlit.io).
3. Click **New app** and select:
   - Repository: your GitHub repo
   - Branch: `main` (or your default branch)
   - Main file path: `streamlit_app.py`
4. Click **Deploy**.
5. Share the generated `.streamlit.app` URL with anyone.

### Notes
- Dependencies are read from `requirements.txt`.
- Streamlit runtime settings are in `.streamlit/config.toml`.
- First launch can take a few minutes while packages install.

---

## ğŸ“Š Expected Outputs

After running the pipeline, you'll have:

### 1. **Frame Metadata** (`frames/metadata.json`)
```json
{
  "extraction_timestamp": "2025-02-26T10:30:00",
  "total_frames": 120,
  "frames": [
    {
      "filename": "video1_frame_0000.jpg",
      "video_id": "video1",
      "timestamp_seconds": 0.0,
      "local_index": 0
    },
    ...
  ]
}
```

### 2. **Embeddings Array** (`embeddings/frame_embeddings.npy`)
- Shape: (N_frames, 512)
- Format: NumPy array (CLIP embeddings)
- Verified: NaN/Inf checks passed âœ“

### 3. **Similarity Report** (`embeddings/similarity_report.json`)
```json
{
  "query_0": {
    "query_frame": "video1_frame_0000.jpg",
    "similar_frames": [
      {"index": 5, "similarity": 0.876, "filename": "video1_frame_0005.jpg"},
      ...
    ]
  },
  ...
}
```

### 4. **Visualizations** (`outputs/`)
- `similarity_heatmap.png` - Frame-to-frame cosine similarity matrix
- `embedding_projection_2d.png` - PCA projection showing mood clusters
- `query_results_bars.png` - Top-5 similar frames per query

---

## ğŸ”¬ API Reference

### `extract_frames.py`

```python
from extract_frames import FrameExtractor

extractor = FrameExtractor(video_dir="videos", output_dir="frames")
saved, filenames = extractor.extract_frames(
    "videos/my_video.mp4",
    frame_interval_seconds=1.0,
    video_id="my_video"
)
extractor.save_metadata("frames/metadata.json")
```

---

### `embed_frames.py`

```python
from embed_frames import FrameEmbedder, save_embeddings

embedder = FrameEmbedder(model_name="openai/clip-vit-base-patch32")
embeddings, metadata = embedder.embed_frames(
    frame_dir="frames",
    metadata_file="frames/metadata.json"
)
save_embeddings(embeddings, metadata, output_dir="embeddings")
```

---

### `similarity_heatmap.py`

```python
from similarity_heatmap import VibeSimilarityAnalyzer

analyzer = VibeSimilarityAnalyzer(
    embeddings_path="embeddings/frame_embeddings.npy",
    metadata_path="embeddings/embeddings_metadata.json"
)

# Get top-k similar frames
similar = analyzer.get_top_k_similar(query_index=0, k=5)
for idx, sim, fname in similar:
    print(f"{fname}: {sim:.3f}")

# Generate report
analyzer.generate_report(query_indices=[0, 50, 100])
analyzer.visualize_heatmap()
```

---

## âœ… Verification & Testing

All outputs are automatically verified for:

### Embedding Verification

```bash
python verification_tests.py
```

Checks:
- âœ“ Shape consistency (N_samples Ã— 512)
- âœ“ No NaN or Inf values
- âœ“ Proper normalization (L2 norm â‰ˆ 1.0)
- âœ“ Self-similarity â‰ˆ 1.0
- âœ“ Embedding diversity (frames have variation)
- âœ“ Duplicate detection

### Similarity Matrix Verification

- âœ“ Symmetry: S[i,j] = S[j,i]
- âœ“ Diagonal = 1.0: Self-similarity
- âœ“ Range: [-1, 1] for cosine similarity
- âœ“ No NaN/Inf propagation

---

## ğŸš€ Performance Metrics

| Component | Latency | Memory | Notes |
|-----------|---------|--------|-------|
| Frame Extraction | ~100ms/10 frames | <500MB | OpenCV on CPU |
| Embedding (1 frame) | 50-150ms | ~1GB | GPU: <50ms |
| Similarity Matrix | <1s | ~50MB | O(NÂ²) distance computation |
| Full Pipeline (50 frames) | ~10s | 2-4GB | GPU recommended |

**Tested On:**
- GPU: NVIDIA RTX 3090 (fastest)
- GPU: CPU only (reasonable inference speed)
- Memory: 8GB RAM sufficient for 100+ frames

---

## ğŸ¯ GenTA Application: Key Insights

### What the Pipeline Reveals

**High Similarity (>0.85)** = Similar mood/style:
- Same lighting/color palette
- Similar emotional tone (energetic, calm, dramatic)
- Comparable aesthetic coherence
- â†’ Predict similar audience response

**Low Similarity (<0.4)** = Contrasting vibes:
- Different emotional appeal
- Distinct visual aesthetics
- Varied production style
- â†’ Predict different engagement/CTR

### Example Use Cases for GenTA

1. **A/B Testing Creative Variants**
   - Upload 2 marketing creatives
   - Compare vibe similarity
   - High similarity â†’ expect similar performance
   - Low similarity â†’ expect different engagement profiles

2. **Mood-Based Creative Search**
   - Query: "Find frames with energetic, vibrant aesthetic"
   - Pipeline returns frames with embedding similarity to reference
   - Curate brand-consistent creative library

3. **Campaign Coherence Scoring**
   - Measure similarity across all ads in campaign
   - High average similarity â†’ consistent brand voice
   - Low similarity â†’ diverse visual strategy

4. **Performance Feedback Loop** (Future)
   - Connect similarity scores to CTR/CVR/ROAS
   - Predict which mood/aesthetic maximizes KPIs
   - Iterate on creative generation informed by mood

---

## ğŸ“ˆ Extending to Full GACS Engine

### Architecture for Next Phase

```
1. Fine-tune CLIP on brand-specific data
   â†“
2. Train regression: Vibe â†’ KPI (CTR/ROAS)
   â†“
3. Real-time creative scoring & ranking
   â†“
4. Feedback loop: Designer input â†’ Model update
   â†“
5. Organizational "aesthetic intelligence"
```

### Required Data for Next Phase
- Creative samples with CTR/CVR/ROAS labels
- Designer feedback (approve/reject signals)
- A/B test results with mood/style annotations

---

## ğŸ› ï¸ Troubleshooting

### Issue: "No modules named 'torch'"

```bash
# Ensure virtual environment is activated
source venv/bin/activate

# Reinstall PyTorch
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Issue: Out of Memory with GPU

```python
# Reduce batch size in embed_frames.py
embedder = FrameEmbedder()
# Process frames in smaller batches
```

### Issue: Video codec not supported

```bash
# Install ffmpeg for better video support
conda install ffmpeg   # or: brew install ffmpeg (macOS)
```

---

## ğŸ“š References & Inspiration

**Models & Frameworks:**
- [CLIP: Learning Transferable Models for Vision](https://arxiv.org/abs/2103.14030) - OpenAI
- [HuggingFace Transformers](https://huggingface.co/models) - Model hub

**Affective Computing:**
- [Sentiment Analysis in Visual Media](#) - GenTA domain focus
- [Color Psychology](https://en.wikipedia.org/wiki/Color_psychology) - Mood aesthetics
- [Visual Hierarchy](https://www.interaction-design.org/literature/topics/visual-hierarchy) - Design principles

---

## ğŸ“œ License & Attribution

**Code:** MIT License (modification-friendly for research)

**Models:** CLIP model from OpenAI (non-commercial research permitted)

**Test Videos:** Provide your own or use public-domain sources:
- [Pexels](https://www.pexels.com/videos/) - CC0 license
- [Pixabay](https://pixabay.com/videos/) - Pixabay License

---

## ğŸ‘¤ Author & Maintenance

**Created For:** GenTA Competency Assessment - AI R&D Engineer

**Engineering Discipline:**
- âœ“ Verification-first design (assertions throughout)
- âœ“ Reproducible & auditable (clear logging)
- âœ“ Production-ready code (error handling, tests)
- âœ“ AI-assisted but human-verified (Copilot-accelerated, fully reviewed)

**Last Updated:** February 2026

---

## ğŸ¤ Contributing & Feedback

### For Future Improvements:
1. Add scene-detection-based frame selection (instead of interval-based)
2. Multimodal fusion (add audio features)
3. Real-time streaming support
4. Web API for creative scoring
5. Dashboard for creative portfolio visualization

---

**âœ“ Ready to extend into GenTA's full GACS affective computing engine.**

Questions? See the Jupyter notebook for detailed explanations: `GenTA_Affective_Computing_Pipeline.ipynb`
